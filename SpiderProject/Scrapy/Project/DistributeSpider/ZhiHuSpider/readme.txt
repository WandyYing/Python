首先创建：
1.scrapy genspider zhihu www.zhihu.com

2.使用账号登录知乎，观察页面元素，并分析要提取哪些元素
  这里设计了两张表问题与答案

3.在items.py文件中添加与表对应的item

4.去页面中获取对应元素属性使用item_load方式加入进spider目录下爬虫文件的parse_detail函数中













2.配置spider中的zhihu.py
##################################################################
  login_zhihu.py这个在新版本知乎上已经不可用，现在登录是通过selenium来完成
    使用requests库来完成登录，详情见login_zhihu.py
    使用登录的原因，是有些数据的获取必须通过登录后才能够获取到
    这个登录属于单独的模块，可以移植到其他网站
      验证码识别还是需要手工（可以想象到为啥打码平台那么火爆），这里只是单纯的把验证码下载下来
##################################################################
      确定要爬取的内容分两个部分：
      1）question ：问题
      2）answer：针对问题的回答，由question的url得到answer的url
      由于这里没有一个所有文章的入口（伯乐在线有总入口），只能通过深度优先的策略进行爬取，这个策略放在parse中，从中筛选符合question的url
      
